{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b927ba-3e85-4167-9526-954d80ac71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jizhicfs/hymiezhao/miniconda3/envs/rl_data_selection/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-10 22:38:36,625\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/jizhicfs/hymiezhao/miniconda3/envs/rl_data_selection/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path=\"/jizhicfs/hymiezhao/models/LUFFY-Qwen-Math-1.5B-Zero\"\n",
    "model_path=\"/jizhicfs/hymiezhao/models/Qwen2.5-Math-1.5B-16k-think\"\n",
    "\n",
    "question = \"which number is larger? 9.11 or 9.9?\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441f0a74-c2f7-49cc-a61b-b365f4fd7089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'which number is larger? 9.11 or 9.9?'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbeffa1-a218-4f27-820a-921620a7552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your task is to follow a systematic, thorough reasoning process before providing the final solution. This involves analyzing, summarizing, exploring, reassessing, and refining your thought process through multiple iterations. Structure your response into two sections: Thought and Solution. In the Thought section, present your reasoning using the format: “<think>\\n {thoughts} </think>\\n”. Each thought should include detailed analysis, brainstorming, verification, and refinement of ideas. After “</think>\\n,” in the Solution section, provide the final, logical, and accurate answer, clearly derived from the exploration in the Thought section. If applicable, include the answer in \\x08oxed{} for closed-form results like multiple choices or mathematical solutions. User: This is the problem:\\nwhich number is larger? 9.11 or 9.9?\\nAssistant: <think>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f575891-b8bb-4d75-9537-9e5571d1bb9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2Model' object has no attribute 'max_position_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\n",
      "File \u001b[0;32m/jizhicfs/hymiezhao/miniconda3/envs/rl_data_selection/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2Model' object has no attribute 'max_position_embeddings'"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33bb7c5d-38ad-4a7e-a0b5-cf6d7fe47874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 22:30:47 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/jizhicfs/hymiezhao/models/Qwen2.5-Math-1.5B-16k-think', speculative_config=None, tokenizer='/jizhicfs/hymiezhao/models/Qwen2.5-Math-1.5B-16k-think', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/jizhicfs/hymiezhao/models/Qwen2.5-Math-1.5B-16k-think, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 05-10 22:30:51 model_runner.py:1060] Starting to load model /jizhicfs/hymiezhao/models/Qwen2.5-Math-1.5B-16k-think...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.37s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.37s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-10 22:30:54 model_runner.py:1071] Loading model weights took 2.8836 GB\n",
      "INFO 05-10 22:30:55 gpu_executor.py:122] # GPU blocks: 155631, # CPU blocks: 9362\n",
      "INFO 05-10 22:30:55 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 151.98x\n",
      "INFO 05-10 22:30:58 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-10 22:30:58 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-10 22:31:09 model_runner.py:1530] Graph capturing finished in 11 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 276.70 toks/s, output: 190"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We can compare the numbers by looking at their decimal parts.\n",
      " {thoughts} <br> \n",
      " 9.9 is larger than 9.11 because the digits in the tenths place of 9.9 are 9, which is greater than the 1 in 9.11.\n",
      "</think>\n",
      " <br> \n",
      " <br> \n",
      " <br> \n",
      " <br> \n",
      " <br> \n",
      " <br> \n",
      " <br> \n",
      " <br> \n",
      " Solution: <color>9.9</color> is larger than 9.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_path)\n",
    "params = SamplingParams(temperature=0.6, max_tokens=8192)\n",
    "outputs = llm.generate([chat], params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dea94-67ed-4533-b863-fdb8bdf8bb34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_reasoning",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
